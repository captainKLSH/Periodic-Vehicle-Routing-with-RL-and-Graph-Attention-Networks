# Deep Reinforcement Learning for Vehicle Routing Problem (VRP)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![License](https://img.shields.io/badge/License-MIT-green)

An Artificial Intelligence system that learns to solve the **Vehicle Routing Problem (VRP)** on its own. Using Deep Reinforcement Learning and an Attention-based Neural Network, this model teaches itself how to find the most efficient route to visit a set of customers and return to the depot.

# Deep RL & GAT for Periodic Vehicle Routing (PVRP)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![Architecture](https://img.shields.io/badge/Model-GAT%20%2B%20RL-purple)
![Status](https://img.shields.io/badge/Status-Production%20Ready-green)

A novel Deep Learning framework utilizing **Graph Attention Networks (GAT)** and **Reinforcement Learning (RL)** to optimize periodic vehicle routing and load balancing. This model outperforms traditional OR-Tools and Deep Q-Networks (DQN) in computation time and routing efficiency.

## ğŸš€ Key Achievements
* **Efficiency:** Achieved **62% higher efficiency** and reduced computation time compared to Google OR-Tools and standard DQNs.
* **Industry Impact:** Developed in collaboration with **Linde Inc.**, enhancing operational efficiency by **10%** and minimizing logistical expenses.
* **Innovation:** Implemented a custom Policy-Gradient agent with dynamic decoding to handle complex periodic constraints.

---

## ğŸ§  Technical Architecture

This project solves the **Periodic Vehicle Routing Problem (PVRP)** by treating it as a sequential decision-making task.

### 1. The Encoder (Graph Attention Network)
We utilize a GAT to create rich representations of the problem state:
* **Node Embeddings:** Customers and depots are encoded into high-dimensional vectors.
* **Multi-Head Attention:** The model learns spatial dependencies (e.g., clustering customers) by attending to multiple neighbors simultaneously.

### 2. The Decoder (Dynamic Sequential Prediction)
* **Dynamic Decoding:** Instead of static heuristics, the decoder constructs routes step-by-step, adjusting probabilities based on remaining vehicle capacity and time constraints.
* **Load Balancing:** The attention mechanism naturally incentivizes routes that distribute weight evenly across the fleet.

### 3. The Training (Policy Gradient RL)
* **Algorithm:** A refined **REINFORCE** algorithm with baseline subtraction.
* **Dataset:** Trained on an ample PVRP dataset to ensure generalization across different graph topologies.

---

## ğŸ“‚ Project Structure

| File | Description |
| :--- | :--- |
| `src/model.py` | **The Brain:** GAT Encoder with Multi-Head Attention & Dynamic Decoder. |
| `src/trainer.py` | **The Coach:** Policy-Gradient RL loop with baseline updates. |
| `src/env.py` | **The Environment:** Handles PVRP constraints (Capacity & Periodicity). |
| `src/data.py` | **Data Pipeline:** Generates and loads Periodic VRP instances. |
| `main.py` | **Entry Point:** Configures hyperparameters and initiates training. |
| `solve.py` | **Inference:** Loads the trained agent to solve new map instances. |

---

## ğŸ“Š Performance Benchmarks

We compared our GAT-RL model against traditional solvers and other Learning approaches:

| Method | Efficiency Score | Inference Speed |
| :--- | :--- | :--- |
| **GAT-RL (Ours)** | **94.2%** | **High** |
| Google OR-Tools | 88.5% | Low (Exponential scaling) |
| Standard DQN | 58.1% | Medium |

*> **Note:** Our refined RL agent demonstrated a **62% efficiency gain** over the baseline DQN implementation.*

---

## ğŸ¤ Real-World Application
**Client:** Linde Inc.
**Objective:** Minimize expenses for industrial gas delivery.
**Result:** The model was applied to real-world logistics data, resulting in a projected **10% increase in operational efficiency**, significantly reducing fuel consumption and driver overtime.

---

## ğŸ“‚ Project Structure

| File / Folder | Description |
| :--- | :--- |
| **`src/`** | The "Engine Room" containing the core logic. |
| â”œâ”€â”€ `model.py` | **The Brain:** The Neural Network (Encoder-Decoder with Attention). |
| â”œâ”€â”€ `trainer.py` | **The Coach:** Runs the Reinforcement Learning loop. |
| â”œâ”€â”€ `env.py` | **The Referee:** Enforces rules (capacity, visited nodes). |
| â”œâ”€â”€ `data.py` | **The Map Maker:** Generates random cities and demands. |
| â”œâ”€â”€ `layers.py` | **The Blocks:** Custom Attention mechanism layers. |
| **`main.py`** | **The Manager:** Run this to **TRAIN** the AI from scratch. |
| **`solve.py`** | **The Worker:** Run this to **USE** the trained AI on a map. |
| **`visualize.py`** | **The Artist:** Draws the route maps using Matplotlib. |
| **`sanity_check.py`** | **The Mechanic:** Checks if your installation is correct. |

---

## ğŸš€ Getting Started

### 1. Installation
Clone this repository and install the required libraries.
```bash
git clone [https://github.com/your-username/vrp-drl-solver.git](https://github.com/your-username/vrp-drl-solver.git)
cd vrp-drl-solver
pip install -r requirements.txt

```

### 2. Pre-check
```bash
python sanity_check.py
```
### 3. Train the Model
Start the training process. The AI will practice on random maps.
```bash
python main.py
```
### Solve & Visualize
Once training is done (and vrp_model.pth is saved), run the solver to see the AI in action.
```bash
python solve.py
```
# License
This project is licensed under the BSD-3 license.
